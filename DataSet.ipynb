{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3272844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "# Display all cell outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "\n",
    "# autoreload extension\n",
    "# if 'autoreload' not in ipython.extension_manager.loaded:\n",
    "#     get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "\n",
    "# get_ipython().run_line_magic('autoreload', '2')\n",
    "\n",
    "# Visualizations\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "raw",
   "id": "00479559",
   "metadata": {},
   "source": [
    "def compute_tf(word_dict, l): # l is the message from logs\n",
    "            tf = {}\n",
    "            sum_nk = len(l)\n",
    "            for word, count in word_dict.items():\n",
    "                try:\n",
    "                    tf[word] = count/sum_nk\n",
    "                except ZeroDivisionError:\n",
    "                    tf[word] = 0\n",
    "            return tf\n",
    "        \n",
    "def compute_idf(strings_list):\n",
    "    n = len(strings_list)\n",
    "    idf = dict.fromkeys(strings_list[0].keys(), 0)\n",
    "    for l in strings_list:\n",
    "        for word, count in l.items():\n",
    "            if count > 0:\n",
    "                idf[word] += 1\n",
    "\n",
    "    for word, v in idf.items():\n",
    "        idf[word] = np.log(n / float(v))\n",
    "    return idf\n",
    "\n",
    "def compute_tf_idf(tf, idf):\n",
    "    tf_idf = dict.fromkeys(tf.keys(), 0)\n",
    "    for word, v in tf.items():\n",
    "        tf_idf[word] = v * idf[word]\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67c9f0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = '07'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fb929a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../File/FrontendFileGroup/storm-frontend-20200307-mask-group.txt\n"
     ]
    }
   ],
   "source": [
    "file_name = f\"../File/FrontendFileGroup/storm-frontend-202003{v}-mask-group.txt\"\n",
    "\n",
    "print('Reading', file_name)\n",
    "logs = pd.read_csv(file_name, index_col=0, nrows = 1e4)\n",
    "tokens_per_message = [x.lower().split() for x in logs.message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8cd1aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = set()\n",
    "\n",
    "for mess in tokens_per_message:\n",
    "    word_set = word_set.union(set(mess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c2e4eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 10000 logs messages, for a total of 9863 unique tokens adopted.\n"
     ]
    }
   ],
   "source": [
    "print(\"We have {} logs messages, for a total of {} unique tokens adopted.\".format(\n",
    "    len(tokens_per_message), len(word_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "695cff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = [dict.fromkeys(word_set, 0) for i in range(len(tokens_per_message))]\n",
    "\n",
    "# Compute raw frequencies of each token per each message\n",
    "for i in range(len(logs.message)):\n",
    "    for word in tokens_per_message[i]:\n",
    "        word_dict[i][word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34e7098b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: there are 0 blanck messages which will be excluded from the analysis.\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for i, dic in enumerate(tokens_per_message):\n",
    "    if not len(dic):\n",
    "        print(i, errors.loc[i])\n",
    "        c += 1\n",
    "\n",
    "print(\"Warning: there are {} blanck messages which will be excluded from the analysis.\".format(c))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "76b1f6f3",
   "metadata": {},
   "source": [
    "tf = [compute_tf(word_dict[i], tokens_per_message[i])\n",
    "      for i in range(len(tokens_per_message))] #if sum(word_dict[i].values())]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f6e17c3e",
   "metadata": {},
   "source": [
    "idf = compute_idf(word_dict)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb358717",
   "metadata": {},
   "source": [
    "tf_idf =  [compute_tf_idf(tf[i], idf) for i in range(len(tf))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c8a2cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the training dataset using a sparse vectorizer\n",
      "done in 0.753497s\n",
      "n_samples: 10000, n_features: 93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract TF-IDF information\n",
    "print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "vectorizer = TfidfVectorizer(max_df=0.8, min_df=0.02, stop_words='english',\n",
    "                             use_idf=True)\n",
    "# vectorizer = TfidfVectorizer(stop_words='english',\n",
    "#                              use_idf=True)\n",
    "X = vectorizer.fit_transform(logs.message)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5aab246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing dimensionality reduction using LSA\n",
      "done in 0.081740s\n"
     ]
    }
   ],
   "source": [
    "# Apply LSA for dimensionality reduction to get a lower-dimensional embedding space\n",
    "print(\"Performing dimensionality reduction using LSA\")\n",
    "t0 = time()\n",
    "\n",
    "# Vectorizer results are normalized, which makes KMeans behave as\n",
    "# spherical k-means for better results. Since LSA/SVD results are\n",
    "# not normalized, we have to redo the normalization.\n",
    "svd = TruncatedSVD(25)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "X = lsa.fit_transform(X)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e54a7252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance of the SVD step: 96%\n"
     ]
    }
   ],
   "source": [
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(\n",
    "      int(explained_variance * 100)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
