{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fefd86af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "# Display all cell outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "\n",
    "# autoreload extension\n",
    "# if 'autoreload' not in ipython.extension_manager.loaded:\n",
    "#     get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "\n",
    "# get_ipython().run_line_magic('autoreload', '2')\n",
    "\n",
    "# Visualizations\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "632f0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(word_dict, l): # l is the message from logs\n",
    "            tf = {}\n",
    "            sum_nk = len(l)\n",
    "            for word, count in word_dict.items():\n",
    "                try:\n",
    "                    tf[word] = count/sum_nk\n",
    "                except ZeroDivisionError:\n",
    "                    tf[word] = 0\n",
    "            return tf\n",
    "        \n",
    "def compute_idf(strings_list):\n",
    "    n = len(strings_list)\n",
    "    idf = dict.fromkeys(strings_list[0].keys(), 0)\n",
    "    for l in strings_list:\n",
    "        for word, count in l.items():\n",
    "            if count > 0:\n",
    "                idf[word] += 1\n",
    "\n",
    "    for word, v in idf.items():\n",
    "        idf[word] = np.log(n / float(v))\n",
    "    return idf\n",
    "\n",
    "def compute_tf_idf(tf, idf):\n",
    "    tf_idf = dict.fromkeys(tf.keys(), 0)\n",
    "    for word, v in tf.items():\n",
    "        tf_idf[word] = v * idf[word]\n",
    "    return tf_idf\n",
    "      \n",
    "\n",
    "def Vectorisation(file_number):\n",
    "    data_set = []\n",
    "    for v in file_number:\n",
    "        file_name = f\"../File/FrontendFileGroup/storm-frontend-202003{v}-mask-group.txt\"\n",
    "\n",
    "        logs = pd.read_csv(file_name, index_col=0, nrows=1e4)\n",
    "        tokens_per_message = [x.lower().split() for x in logs.message]\n",
    "        word_set = set()\n",
    "        \n",
    "        for mess in tokens_per_message:\n",
    "            word_set = word_set.union(set(mess))\n",
    "\n",
    "        print(\"We have {} logs messages, for a total of {} unique tokens adopted.\".format(\n",
    "            len(tokens_per_message), len(word_set)))\n",
    "\n",
    "        word_dict = [dict.fromkeys(word_set, 0) for i in range(len(tokens_per_message))]\n",
    "\n",
    "        # Compute raw frequencies of each token per each message\n",
    "        for i in range(len(logs.message)):\n",
    "            for word in tokens_per_message[i]:\n",
    "                word_dict[i][word] += 1\n",
    "\n",
    "        c = 0\n",
    "        for i, dic in enumerate(tokens_per_message):\n",
    "            if not len(dic):\n",
    "                print(i, errors.loc[i])\n",
    "                c += 1\n",
    "\n",
    "        print(\"Warning: there are {} blanck messages which will be excluded from the analysis.\".format(c))\n",
    "\n",
    "        tf = [compute_tf(word_dict[i], tokens_per_message[i])\n",
    "              for i in range(len(tokens_per_message))] #if sum(word_dict[i].values())]\n",
    "\n",
    "        idf = compute_idf(word_dict)\n",
    "\n",
    "        tf_idf =  [compute_tf_idf(tf[i], idf) for i in range(len(tf))]\n",
    "\n",
    "        # Extract TF-IDF information\n",
    "        print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
    "        t0 = time()\n",
    "        vectorizer = TfidfVectorizer(max_df=0.8, min_df=0.02, stop_words='english',\n",
    "                                     use_idf=True)\n",
    "        # vectorizer = TfidfVectorizer(stop_words='english',\n",
    "        #                              use_idf=True)\n",
    "        X = vectorizer.fit_transform(logs.message)\n",
    "\n",
    "        print(\"done in %fs\" % (time() - t0))\n",
    "        print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "        print()\n",
    "\n",
    "        # Apply LSA for dimensionality reduction to get a lower-dimensional embedding space\n",
    "        print(\"Performing dimensionality reduction using LSA\")\n",
    "        t0 = time()\n",
    "\n",
    "        # Vectorizer results are normalized, which makes KMeans behave as\n",
    "        # spherical k-means for better results. Since LSA/SVD results are\n",
    "        # not normalized, we have to redo the normalization.\n",
    "        svd = TruncatedSVD(25)\n",
    "        normalizer = Normalizer(copy=False)\n",
    "        lsa = make_pipeline(svd, normalizer)\n",
    "        X = lsa.fit_transform(X)\n",
    "\n",
    "        print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "        explained_variance = svd.explained_variance_ratio_.sum()\n",
    "        print(\"Explained variance of the SVD step: {}%\".format(\n",
    "            int(explained_variance * 100)))\n",
    "        \n",
    "        data_set.append(X)\n",
    "        print()\n",
    "    \n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8e1b460",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_number = ['07', '08']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1aed2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 10000 logs messages, for a total of 10124 unique tokens adopted.\n",
      "Warning: there are 0 blanck messages which will be excluded from the analysis.\n",
      "Extracting features from the training dataset using a sparse vectorizer\n"
     ]
    }
   ],
   "source": [
    "data_set = Vectorisation(file_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022f3a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2505db8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanErr = []\n",
    "for v in file_number:\n",
    "    file_name = f\"../File/FrontendFileGroup/storm-frontend-202003{v}-mask-group.txt\"\n",
    "    logs = pd.read_csv(file_name, index_col=0, nrows=1e4)\n",
    "    \n",
    "    X = data_set[file_number.index(v)]\n",
    "    # run K-Means algorithm: 6 clusters\n",
    "    km = KMeans(n_clusters=28, \n",
    "                init='k-means++', \n",
    "                max_iter=500, \n",
    "                n_init=100,\n",
    "#                 verbose=1\n",
    "               )\n",
    "\n",
    "    print(\"Clustering sparse data with %s\" % km)\n",
    "    t0 = time()\n",
    "    km.fit(X)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"We have {} centroids represented as {}-dimensional points.\".format(km.cluster_centers_.shape[0],\n",
    "                                                                              km.cluster_centers_.shape[1]))    \n",
    "    # print the numerosity of each cluster\n",
    "#     print(Counter(km.labels_))\n",
    "\n",
    "    logs[\"kmean_labels\"] = km.labels_\n",
    "\n",
    "    label = np.unique(km.labels_)\n",
    "    count = [Counter(km.labels_)[i] for i in label]\n",
    "\n",
    "    # plot:\n",
    "    fig, ax = plt.subplots(figsize = (8, int(len(label)/3)))\n",
    "\n",
    "    for l, c in zip(label, count):\n",
    "        ax.barh(l, c, linewidth=0.5, edgecolor=\"white\", label=logs.message[logs.kmean_labels==l][0])\n",
    "        ax.text(10, l-0.1, logs.message[logs.kmean_labels==l][0][:80]\n",
    "#                 +'\\n'+logs.message[logs.kmean_labels==l][0][30:60]\n",
    "               )\n",
    "\n",
    "#     with open(f'./BestCentroid/frontend-202003{v}-{n_clusters}centroids.txt', 'w') as fwrite:\n",
    "#         for l in label:\n",
    "#             fwrite.write(f'Label {l}:\\n')\n",
    "#             for i in range(5):\n",
    "#                 fwrite.write(logs.message[logs.kmean_labels==l][i])\n",
    "#                 fwrite.write('\\n')\n",
    "#             fwrite.write('\\n')\n",
    "            \n",
    "    ax.set(yticks=label)\n",
    "\n",
    "    plt.title(f'frontend-202003{v}')\n",
    "    plt.savefig(f'./BestCentroid/frontend-202003{v}', bbox_inches =\"tight\", facecolor='white')\n",
    "    \n",
    "    \n",
    "    import re\n",
    "\n",
    "    error = [0] * len(label)\n",
    "\n",
    "    for l in label:\n",
    "        for msg in logs.message[logs.kmean_labels==l]:\n",
    "            resultE = re.search('error', msg.lower())\n",
    "            resultF = re.search('failure', msg.lower())\n",
    "            result = re.search('SRM_FAILURE', msg)\n",
    "            if resultE!=None :\n",
    "                error[l] += 1\n",
    "            if resultF!=None:\n",
    "                error[l] += 1\n",
    "            if result!=None:\n",
    "                error[l] += 1\n",
    "\n",
    "            \n",
    "    # plot:\n",
    "    fig, ax = plt.subplots(figsize = (8, int(len(label)/3)))\n",
    "    \n",
    "    error_mean = np.mean(error)\n",
    "    for l, c in zip(label, count):\n",
    "        if error[l] > error_mean:\n",
    "            color='red'\n",
    "        else:\n",
    "            color='green'\n",
    "        ax.barh(l, c, linewidth=0.5, color=color, edgecolor=\"white\", label=logs.message[logs.kmean_labels==l][0])\n",
    "        ax.text(10, l-0.2, logs.message[logs.kmean_labels==l][0][:80]\n",
    "#                 +'\\n'+logs.message[logs.kmean_labels==l][0][30:60]\n",
    "               )\n",
    "\n",
    "    # ax.set(xticks=np.arange(0, max(count), 10), yticks=label)\n",
    "    ax.set(yticks=label)\n",
    "    ax.set\n",
    "    # ax.set(xlim=(0, 8), xticks=np.arange(1, 8),\n",
    "    #        ylim=(0, 56), yticks=np.linspace(0, 56, 9))\n",
    "\n",
    "\n",
    "\n",
    "    # ax.legend()\n",
    "    plt.title(f'frontend-202003{v} '+str(error))\n",
    "#     plt.savefig(f'frontend-202003{v}-{n_clusters}centroids', bbox_inches =\"tight\")\n",
    "    plt.show()\n",
    "    meanErr.append(np.mean(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bdeb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanErr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a62e1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(meanErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a60f4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
