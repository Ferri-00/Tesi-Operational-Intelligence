{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "087570bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Display all cell outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "\n",
    "# autoreload extension\n",
    "if 'autoreload' not in ipython.extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "# Visualizations\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db575ee7",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01c76cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fabri\\AppData\\Local\\Temp\\ipykernel_5984\\2175162335.py:4: DtypeWarning: Columns (144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  raw_data = pd.read_csv(file_name, index_col= 0, nrows = 1e4)\n"
     ]
    }
   ],
   "source": [
    "file_name = \"../File/FrontendFileGroup/storm-frontend-20200307-group.txt\"\n",
    "# file_name = \"./storm-frontend-20200307-mask.txt\"\n",
    "\n",
    "raw_data = pd.read_csv(file_name, index_col= 0, nrows = 1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef3a3e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# raw_data = raw_data.drop(raw_data.index[66], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2aa0e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274b7155-f4bc-4649-a0ff-946717c6ea92    False\n",
       "1281ffc9-b3a9-4a6d-af84-02030bc4b4bb    False\n",
       "f4ecc144-881b-4eb4-a456-7563e5bb5209    False\n",
       "ad7c6908-736d-4af0-8c37-e2d4d20b2936    False\n",
       "93fc8683-b4bb-48e3-bbb9-0ffe24c8d81b    False\n",
       "                                        ...  \n",
       "2b4fdb6f-e999-4405-8766-73c06344598a    False\n",
       "e189c148-feb7-43b3-9694-a1c4677c2457    False\n",
       "9d433c1f-0280-4f1a-9ffe-6591ace3517f    False\n",
       "9988999c-73ae-4d09-a082-650698173328    False\n",
       "1fd42fa2-7c7d-4a04-844f-838ff17f893a    False\n",
       "Name: msg180, Length: 10000, dtype: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.msg180.notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc7cc5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fabri\\AppData\\Local\\Temp\\ipykernel_5984\\148899882.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  raw_data['msg1'][i] = message\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m raw_data\u001b[38;5;241m.\u001b[39mindex:\n\u001b[0;32m      4\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m()\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m \u001b[43mraw_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;241m4\u001b[39m:]\u001b[38;5;241m.\u001b[39mdropna():\n\u001b[0;32m      6\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(msg)\n\u001b[0;32m      7\u001b[0m     raw_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmsg1\u001b[39m\u001b[38;5;124m'\u001b[39m][i] \u001b[38;5;241m=\u001b[39m message\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\indexing.py:967\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    964\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    966\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m--> 967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\indexing.py:1202\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1200\u001b[0m \u001b[38;5;66;03m# fall thru to straight lookup\u001b[39;00m\n\u001b[0;32m   1201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m-> 1202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\indexing.py:1153\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m   1151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_label\u001b[39m(\u001b[38;5;28mself\u001b[39m, label, axis: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;66;03m# GH#5667 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[1;32m-> 1153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\generic.py:3896\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[1;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[0;32m   3890\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3891\u001b[0m         \u001b[38;5;66;03m# if we encounter an array-like and we only have 1 dim\u001b[39;00m\n\u001b[0;32m   3892\u001b[0m         \u001b[38;5;66;03m# that means that their are list/ndarrays inside the Series!\u001b[39;00m\n\u001b[0;32m   3893\u001b[0m         \u001b[38;5;66;03m# so just return them (GH 6394)\u001b[39;00m\n\u001b[0;32m   3894\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n\u001b[1;32m-> 3896\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfast_xs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3898\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_sliced(\n\u001b[0;32m   3899\u001b[0m         new_values,\n\u001b[0;32m   3900\u001b[0m         index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns,\n\u001b[0;32m   3901\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex[loc],\n\u001b[0;32m   3902\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mnew_values\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m   3903\u001b[0m     )\n\u001b[0;32m   3904\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_scalar(loc):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\internals\\managers.py:1006\u001b[0m, in \u001b[0;36mBlockManager.fast_xs\u001b[1;34m(self, loc)\u001b[0m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1005\u001b[0m     result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(n, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m-> 1006\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mensure_wrapped_if_datetimelike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;66;03m# Such assignment may incorrectly coerce NaT to None\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m     \u001b[38;5;66;03m# result[blk.mgr_locs] = blk._slice((slice(None), loc))\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, rl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(blk\u001b[38;5;241m.\u001b[39mmgr_locs):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\construction.py:448\u001b[0m, in \u001b[0;36mensure_wrapped_if_datetimelike\u001b[1;34m(arr)\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatetimeArray\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DatetimeArray\u001b[38;5;241m.\u001b[39m_from_sequence(arr)\n\u001b[1;32m--> 448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m:\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimedeltaArray\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TimedeltaArray\u001b[38;5;241m.\u001b[39m_from_sequence(arr)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "errors = pd.DataFrame()\n",
    "\n",
    "for i in raw_data.index:\n",
    "    message = str()\n",
    "    for msg in raw_data.loc[i][4:].dropna():\n",
    "        message += ' ' + str(msg)\n",
    "    raw_data['msg1'][i] = message\n",
    "        #         raw_data.loc[i]['msg1'] += ' ' + msg\n",
    "# for i in raw_data.index:\n",
    "#     lista = raw_data.loc[i]\n",
    "#     idx = lista.pop('id')\n",
    "#     temp = pd.DataFrame(dict(lista), index=[idx], copy=True)\n",
    "#     errors = errors.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f122945",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = raw_data.rename(columns={'msg1':'message'}).drop(raw_data.columns[5:], axis= 1)\n",
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699421df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of errors:\", errors.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982c0a2a",
   "metadata": {},
   "source": [
    "## Extract information\n",
    "Once we read the data, we can focus on the variable message and try to extract meaningful information from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2044d5f",
   "metadata": {},
   "source": [
    "### Clearing and tokenizing messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54b077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to lowercase and split in tokens\n",
    "tokens_per_message = [x.lower().split() for x in errors.message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f74551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the set of all tokens used in the error messages\n",
    "word_set = set()\n",
    "for mess in tokens_per_message:\n",
    "    word_set = word_set.union(set(mess))\n",
    "    \n",
    "word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6898dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have {} error messages, for a total of {} unique tokens adopted.\".format(\n",
    "    len(tokens_per_message), len(word_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976e3b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a dictionary with frequency of all tokens per each message (initialized to 0)\n",
    "word_dict = [dict.fromkeys(word_set, 0) for i in range(len(tokens_per_message))]\n",
    "print(\"Number of tokens:\", len(word_dict[0]))\n",
    "\n",
    "word_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6318782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute raw frequencies of each token per each message\n",
    "for i in range(len(errors.message)):\n",
    "    for word in tokens_per_message[i]:\n",
    "        word_dict[i][word] += 1\n",
    "        \n",
    "word_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848df529",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dictionary dimensions:\\n\", pd.DataFrame(word_dict).shape)\n",
    "\n",
    "# Visualization\n",
    "pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b3f7f8",
   "metadata": {},
   "source": [
    "### Compute TF-IDF scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05024541",
   "metadata": {},
   "source": [
    "Once we have messages divided in words and we have computed the raw frequencies of each token in each sentence, then we can proceed and compute the **tf-idf** score for each message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b7e09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for i, dic in enumerate(tokens_per_message):\n",
    "    if not len(dic):\n",
    "        print(i, errors.loc[i])\n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246bbebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Warning: there are {} blanck messages which will be excluded from the analysis.\".format(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ed91f3",
   "metadata": {},
   "source": [
    "Frequenza delle parole in uno stesso processo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a73a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(word_dict, l):\n",
    "    tf = {}\n",
    "    sum_nk = len(l)\n",
    "    for word, count in word_dict.items():\n",
    "        try:\n",
    "            tf[word] = count/sum_nk\n",
    "        except ZeroDivisionError:\n",
    "            tf[word] = 0\n",
    "    return tf\n",
    "\n",
    "tf = [compute_tf(word_dict[i], tokens_per_message[i])\n",
    "      for i in range(len(tokens_per_message))] #if sum(word_dict[i].values())]\n",
    "# tf_A = compute_tf(word_dict_A, l_A)\n",
    "# tf_B = compute_tf(word_dict_B, l_B)\n",
    "# tf_C = compute_tf(word_dict_C, l_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c961c4e",
   "metadata": {},
   "source": [
    "Frequenza con cui una parola compare nei processi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7fbedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(strings_list):\n",
    "    n = len(strings_list)\n",
    "    idf = dict.fromkeys(strings_list[0].keys(), 0)\n",
    "    for l in strings_list:\n",
    "        for word, count in l.items():\n",
    "            if count > 0:\n",
    "                idf[word] += 1\n",
    "\n",
    "    for word, v in idf.items():\n",
    "        idf[word] = np.log(n / float(v))\n",
    "    return idf\n",
    "\n",
    "idf = compute_idf(word_dict)\n",
    "# idf = compute_idf([word_dict_A, word_dict_B, word_dict_C])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246a7481",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_dict), len(tf), len(tf[0]), len(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4e5737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf_idf(tf, idf):\n",
    "    tf_idf = dict.fromkeys(tf.keys(), 0)\n",
    "    for word, v in tf.items():\n",
    "        tf_idf[word] = v * idf[word]\n",
    "    return tf_idf\n",
    "    \n",
    "tf_idf =  [compute_tf_idf(tf[i], idf) for i in range(len(tf))]\n",
    "# tf_idf_A = compute_tf_idf(tf_A, idf)\n",
    "# tf_idf_B = compute_tf_idf(tf_B, idf)\n",
    "# tf_idf_C = compute_tf_idf(tf_C, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f3681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980e90de",
   "metadata": {},
   "source": [
    "### Alternative using Scikit-Learn: TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc571d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
    "# t0 = time()\n",
    "vectorizer = TfidfVectorizer(max_df=0.8, min_df=0.02, stop_words='english',\n",
    "                             use_idf=True)\n",
    "# vectorizer = TfidfVectorizer(stop_words='english',\n",
    "#                              use_idf=True)\n",
    "X = vectorizer.fit_transform(errors.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8874bd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c55814",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "As a first attempt we can try a simple clustering algorithm such as *K-means*. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> We repeat the information extraction step through the scikit-learn routine TfidfVectorizer just for convenience (so to avoid problems with variable types and to apply easily the clustering algorithm).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b17c171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Extract TF-IDF information\n",
    "print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "vectorizer = TfidfVectorizer(max_df=0.8, min_df=0.02, stop_words='english',\n",
    "                             use_idf=True)\n",
    "# vectorizer = TfidfVectorizer(stop_words='english',\n",
    "#                              use_idf=True)\n",
    "X = vectorizer.fit_transform(errors.message)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "# Apply LSA for dimensionality reduction to get a lower-dimensional embedding space\n",
    "print(\"Performing dimensionality reduction using LSA\")\n",
    "t0 = time()\n",
    "\n",
    "# Vectorizer results are normalized, which makes KMeans behave as\n",
    "# spherical k-means for better results. Since LSA/SVD results are\n",
    "# not normalized, we have to redo the normalization.\n",
    "svd = TruncatedSVD(25)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "X = lsa.fit_transform(X)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(\n",
    "    int(explained_variance * 100)))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427e0962",
   "metadata": {},
   "source": [
    "Once we have a somewhat convenient subspace for representing the words/tokens, and hence the messages, then we cann proceed and apply the clustering algorithm.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> In the following, we pretend the variable <i>category</i> as a true label for the clustering results. This is just to show how one could evaluate results of the unsupervised model when target labels are available.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a1e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# set number of clusters (hyperparameter)\n",
    "n_clusters = 10\n",
    "\n",
    "# run K-Means algorithm: 6 clusters\n",
    "km = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=500, n_init=100,\n",
    "            verbose=1)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(X)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa910d9b",
   "metadata": {},
   "source": [
    "## Results\n",
    "Show graphs and stats here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf16fdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abcda23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('_mpl-gallery')\n",
    "\n",
    "label = np.unique(km.labels_)\n",
    "count = [Counter(km.labels_)[i] for i in label]\n",
    "\n",
    "# plot:\n",
    "fig, ax = plt.subplots(figsize = (6, 4))\n",
    "\n",
    "for l, c in zip(label, count):\n",
    "    ax.barh(l, c, linewidth=0.5, edgecolor=\"white\", label=errors.message[errors.kmean_labels==l][0])\n",
    "\n",
    "ax.set(xticks=np.arange(0, max(count), 10), yticks=label)\n",
    "\n",
    "# ax.set(xlim=(0, 8), xticks=np.arange(1, 8),\n",
    "#        ylim=(0, 56), yticks=np.linspace(0, 56, 9))\n",
    "\n",
    "# ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f63ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in label:\n",
    "    print(errors.message[errors.kmean_labels==l][0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b921f0d",
   "metadata": {},
   "source": [
    "First thing we can look at are the centroids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddba0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have {} centroids represented as {}-dimensional points.\".format(km.cluster_centers_.shape[0], km.cluster_centers_.shape[1]))\n",
    "print(\"Let see the first one for example:\")\n",
    "\n",
    "km.cluster_centers_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ef54ef",
   "metadata": {},
   "source": [
    "Observations labels, instead, can be accessed through the attribute **labels_** of the clustering object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbee636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "# print the numerosity of each cluster\n",
    "print(Counter(km.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca6ccc5",
   "metadata": {},
   "source": [
    "Now we can look for example to the messages that fall into the same class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d52bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors[\"kmean_labels\"] = km.labels_\n",
    "\n",
    "for msg in errors.message[errors.kmean_labels==8]:\n",
    "    print(msg, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aac1d1",
   "metadata": {},
   "source": [
    "### Check the true label (if available)\n",
    "\n",
    "Questa parte servirebbe se sapessi a priori a quale categoria appartiene ogni processo per controllare che l'algoritmo abbia prodotto i risultati attesi, ma non è questo il caso"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d37521a9",
   "metadata": {},
   "source": [
    "# extract true clusters\n",
    "labels = errors.category\n",
    "\n",
    "# evaluate results comparing with labels\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b70d98d5",
   "metadata": {},
   "source": [
    "print(\"There are {} unique categories for error messages.\".format(len(np.unique(labels))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9cdcad",
   "metadata": {},
   "source": [
    "## Conclusions and Next Steps\n",
    "Summarize findings here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
