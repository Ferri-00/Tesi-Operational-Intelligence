{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fefd86af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "# Display all cell outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "\n",
    "# autoreload extension\n",
    "# if 'autoreload' not in ipython.extension_manager.loaded:\n",
    "#     get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "\n",
    "# get_ipython().run_line_magic('autoreload', '2')\n",
    "\n",
    "# Visualizations\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "632f0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid(file_number, start, stop, step):\n",
    "    for v in file_number:\n",
    "        file_name = f\"../File/FrontendFileGroup/storm-frontend-202003{v}-mask-group.txt\"\n",
    "        # file_name = f\"../File/FrontendFileGroup/storm-frontend-202003{v}-group.txt\"\n",
    "\n",
    "        logs = pd.read_csv(file_name, index_col=0, nrows=1e4)\n",
    "\n",
    "        tokens_per_message = [x.lower().split() for x in logs.message]\n",
    "\n",
    "        word_set = set()\n",
    "        for mess in tokens_per_message:\n",
    "            word_set = word_set.union(set(mess))\n",
    "\n",
    "        print(\"We have {} logs messages, for a total of {} unique tokens adopted.\".format(\n",
    "            len(tokens_per_message), len(word_set)))\n",
    "\n",
    "\n",
    "        word_dict = [dict.fromkeys(word_set, 0) for i in range(len(tokens_per_message))]\n",
    "\n",
    "        # Compute raw frequencies of each token per each message\n",
    "        for i in range(len(logs.message)):\n",
    "            for word in tokens_per_message[i]:\n",
    "                word_dict[i][word] += 1\n",
    "\n",
    "        c = 0\n",
    "        for i, dic in enumerate(tokens_per_message):\n",
    "            if not len(dic):\n",
    "                print(i, errors.loc[i])\n",
    "                c += 1\n",
    "\n",
    "        print(\"Warning: there are {} blanck messages which will be excluded from the analysis.\".format(c))\n",
    "\n",
    "        def compute_tf(word_dict, l): # l is the message from logs\n",
    "            tf = {}\n",
    "            sum_nk = len(l)\n",
    "            for word, count in word_dict.items():\n",
    "                try:\n",
    "                    tf[word] = count/sum_nk\n",
    "                except ZeroDivisionError:\n",
    "                    tf[word] = 0\n",
    "            return tf\n",
    "\n",
    "        tf = [compute_tf(word_dict[i], tokens_per_message[i])\n",
    "              for i in range(len(tokens_per_message))] #if sum(word_dict[i].values())]\n",
    "\n",
    "        def compute_idf(strings_list):\n",
    "            n = len(strings_list)\n",
    "            idf = dict.fromkeys(strings_list[0].keys(), 0)\n",
    "            for l in strings_list:\n",
    "                for word, count in l.items():\n",
    "                    if count > 0:\n",
    "                        idf[word] += 1\n",
    "\n",
    "            for word, v in idf.items():\n",
    "                idf[word] = np.log(n / float(v))\n",
    "            return idf\n",
    "\n",
    "        idf = compute_idf(word_dict)\n",
    "\n",
    "        def compute_tf_idf(tf, idf):\n",
    "            tf_idf = dict.fromkeys(tf.keys(), 0)\n",
    "            for word, v in tf.items():\n",
    "                tf_idf[word] = v * idf[word]\n",
    "            return tf_idf\n",
    "\n",
    "        tf_idf =  [compute_tf_idf(tf[i], idf) for i in range(len(tf))]\n",
    "\n",
    "        # Extract TF-IDF information\n",
    "        print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
    "        t0 = time()\n",
    "        vectorizer = TfidfVectorizer(max_df=0.8, min_df=0.02, stop_words='english',\n",
    "                                     use_idf=True)\n",
    "        # vectorizer = TfidfVectorizer(stop_words='english',\n",
    "        #                              use_idf=True)\n",
    "        X = vectorizer.fit_transform(logs.message)\n",
    "\n",
    "        print(\"done in %fs\" % (time() - t0))\n",
    "        print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "        print()\n",
    "\n",
    "        # Apply LSA for dimensionality reduction to get a lower-dimensional embedding space\n",
    "        print(\"Performing dimensionality reduction using LSA\")\n",
    "        t0 = time()\n",
    "\n",
    "        # Vectorizer results are normalized, which makes KMeans behave as\n",
    "        # spherical k-means for better results. Since LSA/SVD results are\n",
    "        # not normalized, we have to redo the normalization.\n",
    "        svd = TruncatedSVD(25)\n",
    "        normalizer = Normalizer(copy=False)\n",
    "        lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "        X = lsa.fit_transform(X)\n",
    "\n",
    "        print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "        explained_variance = svd.explained_variance_ratio_.sum()\n",
    "        print(\"Explained variance of the SVD step: {}%\".format(\n",
    "            int(explained_variance * 100)))\n",
    "\n",
    "        print()\n",
    "        \n",
    "        labels = np.arange(0, stop)\n",
    "        counts = []\n",
    "        cluster = []\n",
    "        \n",
    "        for n_clusters in range(start, stop, step):\n",
    "            cluster += [n_clusters]\n",
    "            # run K-Means algorithm: 6 clusters\n",
    "            km = KMeans(n_clusters=n_clusters, \n",
    "                        init='k-means++', \n",
    "                        max_iter=500, \n",
    "                        n_init=100,\n",
    "#                         verbose=1\n",
    "                       )\n",
    "\n",
    "            print(\"Clustering sparse data with %s\" % km)\n",
    "            t0 = time()\n",
    "            km.fit(X)\n",
    "            print(\"done in %0.3fs\" % (time() - t0))\n",
    "            print()\n",
    "        \n",
    "            print(\"We have {} centroids represented as {}-dimensional points.\".format(km.cluster_centers_.shape[0],\n",
    "                                                                                      km.cluster_centers_.shape[1]))\n",
    "            # print the numerosity of each cluster\n",
    "            print(Counter(km.labels_))\n",
    "\n",
    "            logs[\"kmean_labels\"] = km.labels_\n",
    "\n",
    "            label = np.unique(km.labels_)\n",
    "            count = [Counter(km.labels_)[i] for i in label]\n",
    "    \n",
    "            plt.style.use('_mpl-gallery')\n",
    "\n",
    "            # plot:\n",
    "            fig, ax = plt.subplots(figsize = (8, n_clusters/2))\n",
    "\n",
    "            for l, c in zip(label, count):\n",
    "                ax.barh(l, c, linewidth=0.5, edgecolor=\"white\", label=logs.message[logs.kmean_labels==l][0])\n",
    "                ax.text(c, l-0.35, logs.message[logs.kmean_labels==l][0][:30]+'\\n'+logs.message[logs.kmean_labels==l][0][30:60])\n",
    "\n",
    "            ax.set(yticks=label)\n",
    "\n",
    "            plt.title(f'frontend-202003{v}-{n_clusters}centroids')\n",
    "                        \n",
    "            counts += [np.sort(count)]\n",
    "                        \n",
    "            plt.savefig(f'frontend-202003{v}-{n_clusters}centroids', bbox_inches =\"tight\")\n",
    "\n",
    "        # plot:\n",
    "        fig, ax = plt.subplots(figsize = (8, 5))\n",
    "        color = list(mcolors.CSS4_COLORS)\n",
    "#         color = list(mcolors.TABLEAU_COLORS)\n",
    "        \n",
    "        print('complete counts for all labels')\n",
    "        for c in range(len(counts)):\n",
    "            while len(labels) > len(counts[c]):\n",
    "                counts[c] += [0]\n",
    "        print('plot variation of granularity of plots')       \n",
    "        \n",
    "        print('lunghezza cluster:', len(cluster))\n",
    "        print('lunghezza counts:', len(counts))\n",
    "        print('lunghezza singoli counts:', len(counts[0]))\n",
    "\n",
    "        col=0\n",
    "        for l in labels:\n",
    "            \n",
    "            ax.plot(cluster, [counts[i][l] for i in range(len(cluster))], '-', color=color[col], label=str(l)+' '+color[col])\n",
    "            col+=1\n",
    "\n",
    "        ax.set(xticks=label)\n",
    "\n",
    "        ax.legend()\n",
    "        plt.title(f'BestCentroid/frontend-202003{v}-best_centroids-{start}-{stop}-{step}')\n",
    "        plt.savefig(f'BestCentroid/frontend-202003{v}_best_centroids', bbox_inches =\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bac952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 10000 logs messages, for a total of 10113 unique tokens adopted.\n",
      "Warning: there are 0 blanck messages which will be excluded from the analysis.\n",
      "Extracting features from the training dataset using a sparse vectorizer\n",
      "done in 0.860557s\n",
      "n_samples: 10000, n_features: 93\n",
      "\n",
      "Performing dimensionality reduction using LSA\n",
      "done in 0.124999s\n",
      "Explained variance of the SVD step: 96%\n",
      "\n",
      "Clustering sparse data with KMeans(max_iter=500, n_clusters=10, n_init=100)\n",
      "done in 1.779s\n",
      "\n",
      "We have 10 centroids represented as 25-dimensional points.\n",
      "Counter({1: 3191, 4: 1224, 3: 927, 5: 897, 0: 832, 8: 771, 9: 635, 6: 591, 7: 472, 2: 460})\n",
      "Clustering sparse data with KMeans(max_iter=500, n_clusters=15, n_init=100)\n",
      "done in 2.533s\n",
      "\n",
      "We have 15 centroids represented as 25-dimensional points.\n",
      "Counter({0: 3191, 8: 1227, 7: 771, 10: 700, 2: 657, 1: 591, 4: 483, 9: 453, 13: 386, 12: 350, 14: 262, 3: 251, 6: 234, 11: 227, 5: 217})\n",
      "Clustering sparse data with KMeans(max_iter=500, n_clusters=20, n_init=100)\n",
      "done in 2.791s\n",
      "\n",
      "We have 20 centroids represented as 25-dimensional points.\n",
      "Counter({0: 3191, 6: 1218, 5: 771, 1: 702, 4: 483, 8: 453, 11: 383, 7: 366, 19: 341, 10: 327, 3: 262, 12: 251, 16: 234, 9: 217, 2: 208, 14: 201, 15: 140, 13: 100, 18: 81, 17: 71})\n",
      "Clustering sparse data with KMeans(max_iter=500, n_clusters=25, n_init=100)\n",
      "done in 3.505s\n",
      "\n",
      "We have 25 centroids represented as 25-dimensional points.\n",
      "Counter({1: 3191, 3: 1210, 4: 771, 9: 591, 14: 483, 7: 383, 2: 360, 0: 335, 5: 307, 6: 251, 12: 238, 17: 217, 18: 208, 10: 203, 13: 203, 21: 185, 24: 176, 23: 116, 22: 106, 19: 102, 8: 93, 20: 75, 16: 71, 11: 64, 15: 61})\n",
      "complete counts for all labels\n"
     ]
    }
   ],
   "source": [
    "centroid(['07'], 10, 30, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
